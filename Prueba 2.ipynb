{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\denia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import numpy as np    \n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arabic',\n",
       " 'azerbaijani',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'greek',\n",
       " 'hungarian',\n",
       " 'indonesian',\n",
       " 'italian',\n",
       " 'kazakh',\n",
       " 'nepali',\n",
       " 'norwegian',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'slovene',\n",
       " 'spanish',\n",
       " 'swedish',\n",
       " 'tajik',\n",
       " 'turkish']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'al',\n",
       " 'algo',\n",
       " 'algunas',\n",
       " 'algunos',\n",
       " 'ante',\n",
       " 'antes',\n",
       " 'como',\n",
       " 'con',\n",
       " 'contra',\n",
       " 'cual',\n",
       " 'cuando',\n",
       " 'de',\n",
       " 'del',\n",
       " 'desde',\n",
       " 'donde',\n",
       " 'durante',\n",
       " 'e',\n",
       " 'el',\n",
       " 'ella',\n",
       " 'ellas',\n",
       " 'ellos',\n",
       " 'en',\n",
       " 'entre',\n",
       " 'era',\n",
       " 'erais',\n",
       " 'eran',\n",
       " 'eras',\n",
       " 'eres',\n",
       " 'es',\n",
       " 'esa',\n",
       " 'esas',\n",
       " 'ese',\n",
       " 'eso',\n",
       " 'esos',\n",
       " 'esta',\n",
       " 'estaba',\n",
       " 'estabais',\n",
       " 'estaban',\n",
       " 'estabas',\n",
       " 'estad',\n",
       " 'estada',\n",
       " 'estadas',\n",
       " 'estado',\n",
       " 'estados',\n",
       " 'estamos',\n",
       " 'estando',\n",
       " 'estar',\n",
       " 'estaremos',\n",
       " 'estará',\n",
       " 'estarán',\n",
       " 'estarás',\n",
       " 'estaré',\n",
       " 'estaréis',\n",
       " 'estaría',\n",
       " 'estaríais',\n",
       " 'estaríamos',\n",
       " 'estarían',\n",
       " 'estarías',\n",
       " 'estas',\n",
       " 'este',\n",
       " 'estemos',\n",
       " 'esto',\n",
       " 'estos',\n",
       " 'estoy',\n",
       " 'estuve',\n",
       " 'estuviera',\n",
       " 'estuvierais',\n",
       " 'estuvieran',\n",
       " 'estuvieras',\n",
       " 'estuvieron',\n",
       " 'estuviese',\n",
       " 'estuvieseis',\n",
       " 'estuviesen',\n",
       " 'estuvieses',\n",
       " 'estuvimos',\n",
       " 'estuviste',\n",
       " 'estuvisteis',\n",
       " 'estuviéramos',\n",
       " 'estuviésemos',\n",
       " 'estuvo',\n",
       " 'está',\n",
       " 'estábamos',\n",
       " 'estáis',\n",
       " 'están',\n",
       " 'estás',\n",
       " 'esté',\n",
       " 'estéis',\n",
       " 'estén',\n",
       " 'estés',\n",
       " 'fue',\n",
       " 'fuera',\n",
       " 'fuerais',\n",
       " 'fueran',\n",
       " 'fueras',\n",
       " 'fueron',\n",
       " 'fuese',\n",
       " 'fueseis',\n",
       " 'fuesen',\n",
       " 'fueses',\n",
       " 'fui',\n",
       " 'fuimos',\n",
       " 'fuiste',\n",
       " 'fuisteis',\n",
       " 'fuéramos',\n",
       " 'fuésemos',\n",
       " 'ha',\n",
       " 'habida',\n",
       " 'habidas',\n",
       " 'habido',\n",
       " 'habidos',\n",
       " 'habiendo',\n",
       " 'habremos',\n",
       " 'habrá',\n",
       " 'habrán',\n",
       " 'habrás',\n",
       " 'habré',\n",
       " 'habréis',\n",
       " 'habría',\n",
       " 'habríais',\n",
       " 'habríamos',\n",
       " 'habrían',\n",
       " 'habrías',\n",
       " 'habéis',\n",
       " 'había',\n",
       " 'habíais',\n",
       " 'habíamos',\n",
       " 'habían',\n",
       " 'habías',\n",
       " 'han',\n",
       " 'has',\n",
       " 'hasta',\n",
       " 'hay',\n",
       " 'haya',\n",
       " 'hayamos',\n",
       " 'hayan',\n",
       " 'hayas',\n",
       " 'hayáis',\n",
       " 'he',\n",
       " 'hemos',\n",
       " 'hube',\n",
       " 'hubiera',\n",
       " 'hubierais',\n",
       " 'hubieran',\n",
       " 'hubieras',\n",
       " 'hubieron',\n",
       " 'hubiese',\n",
       " 'hubieseis',\n",
       " 'hubiesen',\n",
       " 'hubieses',\n",
       " 'hubimos',\n",
       " 'hubiste',\n",
       " 'hubisteis',\n",
       " 'hubiéramos',\n",
       " 'hubiésemos',\n",
       " 'hubo',\n",
       " 'la',\n",
       " 'las',\n",
       " 'le',\n",
       " 'les',\n",
       " 'lo',\n",
       " 'los',\n",
       " 'me',\n",
       " 'mi',\n",
       " 'mis',\n",
       " 'mucho',\n",
       " 'muchos',\n",
       " 'muy',\n",
       " 'más',\n",
       " 'mí',\n",
       " 'mía',\n",
       " 'mías',\n",
       " 'mío',\n",
       " 'míos',\n",
       " 'nada',\n",
       " 'ni',\n",
       " 'no',\n",
       " 'nos',\n",
       " 'nosotras',\n",
       " 'nosotros',\n",
       " 'nuestra',\n",
       " 'nuestras',\n",
       " 'nuestro',\n",
       " 'nuestros',\n",
       " 'o',\n",
       " 'os',\n",
       " 'otra',\n",
       " 'otras',\n",
       " 'otro',\n",
       " 'otros',\n",
       " 'para',\n",
       " 'pero',\n",
       " 'poco',\n",
       " 'por',\n",
       " 'porque',\n",
       " 'que',\n",
       " 'quien',\n",
       " 'quienes',\n",
       " 'qué',\n",
       " 'se',\n",
       " 'sea',\n",
       " 'seamos',\n",
       " 'sean',\n",
       " 'seas',\n",
       " 'sentid',\n",
       " 'sentida',\n",
       " 'sentidas',\n",
       " 'sentido',\n",
       " 'sentidos',\n",
       " 'seremos',\n",
       " 'será',\n",
       " 'serán',\n",
       " 'serás',\n",
       " 'seré',\n",
       " 'seréis',\n",
       " 'sería',\n",
       " 'seríais',\n",
       " 'seríamos',\n",
       " 'serían',\n",
       " 'serías',\n",
       " 'seáis',\n",
       " 'siente',\n",
       " 'sin',\n",
       " 'sintiendo',\n",
       " 'sobre',\n",
       " 'sois',\n",
       " 'somos',\n",
       " 'son',\n",
       " 'soy',\n",
       " 'su',\n",
       " 'sus',\n",
       " 'suya',\n",
       " 'suyas',\n",
       " 'suyo',\n",
       " 'suyos',\n",
       " 'sí',\n",
       " 'también',\n",
       " 'tanto',\n",
       " 'te',\n",
       " 'tendremos',\n",
       " 'tendrá',\n",
       " 'tendrán',\n",
       " 'tendrás',\n",
       " 'tendré',\n",
       " 'tendréis',\n",
       " 'tendría',\n",
       " 'tendríais',\n",
       " 'tendríamos',\n",
       " 'tendrían',\n",
       " 'tendrías',\n",
       " 'tened',\n",
       " 'tenemos',\n",
       " 'tenga',\n",
       " 'tengamos',\n",
       " 'tengan',\n",
       " 'tengas',\n",
       " 'tengo',\n",
       " 'tengáis',\n",
       " 'tenida',\n",
       " 'tenidas',\n",
       " 'tenido',\n",
       " 'tenidos',\n",
       " 'teniendo',\n",
       " 'tenéis',\n",
       " 'tenía',\n",
       " 'teníais',\n",
       " 'teníamos',\n",
       " 'tenían',\n",
       " 'tenías',\n",
       " 'ti',\n",
       " 'tiene',\n",
       " 'tienen',\n",
       " 'tienes',\n",
       " 'todo',\n",
       " 'todos',\n",
       " 'tu',\n",
       " 'tus',\n",
       " 'tuve',\n",
       " 'tuviera',\n",
       " 'tuvierais',\n",
       " 'tuvieran',\n",
       " 'tuvieras',\n",
       " 'tuvieron',\n",
       " 'tuviese',\n",
       " 'tuvieseis',\n",
       " 'tuviesen',\n",
       " 'tuvieses',\n",
       " 'tuvimos',\n",
       " 'tuviste',\n",
       " 'tuvisteis',\n",
       " 'tuviéramos',\n",
       " 'tuviésemos',\n",
       " 'tuvo',\n",
       " 'tuya',\n",
       " 'tuyas',\n",
       " 'tuyo',\n",
       " 'tuyos',\n",
       " 'tú',\n",
       " 'un',\n",
       " 'una',\n",
       " 'uno',\n",
       " 'unos',\n",
       " 'vosotras',\n",
       " 'vosotros',\n",
       " 'vuestra',\n",
       " 'vuestras',\n",
       " 'vuestro',\n",
       " 'vuestros',\n",
       " 'y',\n",
       " 'ya',\n",
       " 'yo',\n",
       " 'él',\n",
       " 'éramos'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(stopwords.words('spanish'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimento1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preface Automated Machine Learning Methods, Systems, Challenges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Párrafo 1\n",
    "text_01 = \"¡Se une al #DíaSinMujeres! La cadena estadounidense Hooters publicó que contará con servicio de comida para llevar y servicio a domicilio, este último mediante Uber Eats\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Párrafo 2\n",
    "text_02 = \"En vez de darles el día deberían dejar de cosificar a sus empleadas.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Párrafo 3\n",
    "text_03 = \"Contradictorio que apoye el paro de mujeres Rostro pensativo pero pida que su personal femenino utilice esos mini shorts y blusas scotadas por ahí debería empezar su cambio \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimento 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://twitter.com/sopitas/status/1236098576238997504 \n",
    "text_04 = \"La Selección Femenil de futbol le ganó a Haití y obtuvo su pase al mundial ¡Muchas felicidades! Poniendo el futbol de nuestro país en alto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://twitter.com/meelycampos/status/1233640560444854272\n",
    "text_05 = \"Tengo 1 año 5 meses con mi novio y nunca ha querido subir una foto conmigo a ninguna red social. Cada like es un amiga date cuenta.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://twitter.com/eileenOrnot/status/1233435752937246720\n",
    "text_06 = \"La tasa de mortalidad por feminicidio creció 138%. La del coronavirus es del 2%. O sea que si hoy en la noche te besuquearas a un guey infectado tienes más posibilidades de morir asesinada por el tipo que por el virus. Buenos días, República Mexicana.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-89-e432856e9b1a>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-89-e432856e9b1a>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    for documento in df.(text)\u001b[0m\n\u001b[1;37m                        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def function(df, idioma):\n",
    "    contador = 0\n",
    "    for documento in df\n",
    "    text_tokens = tokenizer.tokenizer(documento.lower())\n",
    "    text_tokens_wout_stopwords =[]\n",
    "    for word in text_tokens: \n",
    "        dicc_text.updead([text_ + contador: text_tokens_wout_stopwords]\n",
    "        if word not in stopwords.words(idioma):\n",
    "            text_tokens_wout_stopwords.apped()\n",
    "            contador = contador +1 \n",
    "                         return dicc_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['se',\n",
       " 'une',\n",
       " 'al',\n",
       " 'díasinmujeres',\n",
       " 'la',\n",
       " 'cadena',\n",
       " 'estadounidense',\n",
       " 'hooters',\n",
       " 'publicó',\n",
       " 'que',\n",
       " 'contará',\n",
       " 'con',\n",
       " 'servicio',\n",
       " 'de',\n",
       " 'comida',\n",
       " 'para',\n",
       " 'llevar',\n",
       " 'y',\n",
       " 'servicio',\n",
       " 'a',\n",
       " 'domicilio',\n",
       " 'este',\n",
       " 'último',\n",
       " 'mediante',\n",
       " 'uber',\n",
       " 'eats']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list=[text_01, text_02,text_03,text_04,text_05,text_06]\n",
    "\n",
    "j=len(list)\n",
    "\n",
    "def token(x): \n",
    "    for i in range(j):\n",
    "        text=x[i];\n",
    "        text_tokens=[];\n",
    "        text_tokens = tokenizer.tokenize(text.lower());#tokenizar y quitar signos de puntuación\n",
    "#print(text_01_tokens)\n",
    "        return(text_tokens)\n",
    "        #text_tokens_wout_stopwords = [];\n",
    "        #for word in text_tokens:\n",
    "         #   if word not in stopwords.words('spanish'): \n",
    "          #      text_tokens_wout_stopwords.append(word)\n",
    "        #     return(text_tokens_wout_stopwords)\n",
    "                \n",
    "            \n",
    "token(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', 'año', '5', 'meses', 'novio', 'nunca', 'querido', 'subir', 'foto', 'conmigo', 'ninguna', 'red', 'social', 'cada', 'like', 'amiga', 'date', 'cuenta']\n"
     ]
    }
   ],
   "source": [
    "text_02_tokens = tokenizer.tokenize(text_02.lower()) \n",
    "#print(text_02_tokens)\n",
    "\n",
    "text_02_tokens_wout_stopwords = []\n",
    "\n",
    "for word in text_02_tokens:\n",
    "    if word not in stopwords.words('spanish'): text_02_tokens_wout_stopwords.append(word)\n",
    "\n",
    "print(text_02_tokens_wout_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tasa', 'mortalidad', 'feminicidio', 'creció', '138', 'coronavirus', '2', 'si', 'hoy', 'noche', 'besuquearas', 'guey', 'infectado', 'posibilidades', 'morir', 'asesinada', 'tipo', 'virus', 'buenos', 'días', 'república', 'mexicana']\n"
     ]
    }
   ],
   "source": [
    "text_03_tokens = tokenizer.tokenize(text_03.lower()) \n",
    "#print(text_03_tokens)\n",
    "\n",
    "text_03_tokens_wout_stopwords = []\n",
    "\n",
    "for word in text_03_tokens:\n",
    "    if word not in stopwords.words('spanish'): text_03_tokens_wout_stopwords.append(word)\n",
    "\n",
    "print(text_03_tokens_wout_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "18\n",
      "22\n",
      "46\n"
     ]
    }
   ],
   "source": [
    "print(len(text_01_tokens_wout_stopwords))\n",
    "print(len(text_02_tokens_wout_stopwords))\n",
    "print(len(text_03_tokens_wout_stopwords))\n",
    "print(len(text_01_tokens_wout_stopwords) + len(text_02_tokens_wout_stopwords) + len(text_01_tokens_wout_stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de la BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicc_texts = {\"text_01\": text_01_tokens_wout_stopwords, \n",
    " \"text_02\": text_02_tokens_wout_stopwords, \n",
    " \"text_03\": text_03_tokens_wout_stopwords}\n",
    "#dicc_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'selección': 1,\n",
       " 'femenil': 1,\n",
       " 'futbol': 2,\n",
       " 'ganó': 1,\n",
       " 'haití': 1,\n",
       " 'obtuvo': 1,\n",
       " 'pase': 1,\n",
       " 'mundial': 1,\n",
       " 'muchas': 1,\n",
       " 'felicidades': 1,\n",
       " 'poniendo': 1,\n",
       " 'país': 1,\n",
       " 'alto': 1,\n",
       " '1': 1,\n",
       " 'año': 1,\n",
       " '5': 1,\n",
       " 'meses': 1,\n",
       " 'novio': 1,\n",
       " 'nunca': 1,\n",
       " 'querido': 1,\n",
       " 'subir': 1,\n",
       " 'foto': 1,\n",
       " 'conmigo': 1,\n",
       " 'ninguna': 1,\n",
       " 'red': 1,\n",
       " 'social': 1,\n",
       " 'cada': 1,\n",
       " 'like': 1,\n",
       " 'amiga': 1,\n",
       " 'date': 1,\n",
       " 'cuenta': 1,\n",
       " 'tasa': 1,\n",
       " 'mortalidad': 1,\n",
       " 'feminicidio': 1,\n",
       " 'creció': 1,\n",
       " '138': 1,\n",
       " 'coronavirus': 1,\n",
       " '2': 1,\n",
       " 'si': 1,\n",
       " 'hoy': 1,\n",
       " 'noche': 1,\n",
       " 'besuquearas': 1,\n",
       " 'guey': 1,\n",
       " 'infectado': 1,\n",
       " 'posibilidades': 1,\n",
       " 'morir': 1,\n",
       " 'asesinada': 1,\n",
       " 'tipo': 1,\n",
       " 'virus': 1,\n",
       " 'buenos': 1,\n",
       " 'días': 1,\n",
       " 'república': 1,\n",
       " 'mexicana': 1}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicc_termns = {}\n",
    "\n",
    "for text in dicc_texts:\n",
    "    for word in dicc_texts[text]:\n",
    "        \n",
    "#        print(\"EVALUAR:\", word, \"EN\", text)\n",
    "        \n",
    "        if(word in dicc_termns):#incrementar palabras al diccionario\n",
    "            dicc_termns[word] = dicc_termns[word] + 1\n",
    "            \n",
    "#            print(word, \"IN\", \"dicc_termns\")\n",
    "            \n",
    "        elif(word not in dicc_termns):#agregar palabras al diccionario        \n",
    "            dicc_termns[word] = 1\n",
    "            \n",
    "#            print(word, \"NOT IN\", \"dicc_termns\")            \n",
    "\n",
    "print(len(dicc_termns))\n",
    "dicc_termns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matriz Término Documento (binaria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = np.zeros((len(dicc_texts), len(dicc_termns))) # Pre-allocate matrix\n",
    "#matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selección IN text_01\n",
      "se agregó:  1.0 en:  0 0\n",
      "selección NOT IN text_02\n",
      "se agregó:  0.0 en:  1 0\n",
      "selección NOT IN text_03\n",
      "se agregó:  0.0 en:  2 0\n",
      "femenil IN text_01\n",
      "se agregó:  1.0 en:  0 1\n",
      "femenil NOT IN text_02\n",
      "se agregó:  0.0 en:  1 1\n",
      "femenil NOT IN text_03\n",
      "se agregó:  0.0 en:  2 1\n",
      "futbol IN text_01\n",
      "se agregó:  1.0 en:  0 2\n",
      "futbol NOT IN text_02\n",
      "se agregó:  0.0 en:  1 2\n",
      "futbol NOT IN text_03\n",
      "se agregó:  0.0 en:  2 2\n",
      "ganó IN text_01\n",
      "se agregó:  1.0 en:  0 3\n",
      "ganó NOT IN text_02\n",
      "se agregó:  0.0 en:  1 3\n",
      "ganó NOT IN text_03\n",
      "se agregó:  0.0 en:  2 3\n",
      "haití IN text_01\n",
      "se agregó:  1.0 en:  0 4\n",
      "haití NOT IN text_02\n",
      "se agregó:  0.0 en:  1 4\n",
      "haití NOT IN text_03\n",
      "se agregó:  0.0 en:  2 4\n",
      "obtuvo IN text_01\n",
      "se agregó:  1.0 en:  0 5\n",
      "obtuvo NOT IN text_02\n",
      "se agregó:  0.0 en:  1 5\n",
      "obtuvo NOT IN text_03\n",
      "se agregó:  0.0 en:  2 5\n",
      "pase IN text_01\n",
      "se agregó:  1.0 en:  0 6\n",
      "pase NOT IN text_02\n",
      "se agregó:  0.0 en:  1 6\n",
      "pase NOT IN text_03\n",
      "se agregó:  0.0 en:  2 6\n",
      "mundial IN text_01\n",
      "se agregó:  1.0 en:  0 7\n",
      "mundial NOT IN text_02\n",
      "se agregó:  0.0 en:  1 7\n",
      "mundial NOT IN text_03\n",
      "se agregó:  0.0 en:  2 7\n",
      "muchas IN text_01\n",
      "se agregó:  1.0 en:  0 8\n",
      "muchas NOT IN text_02\n",
      "se agregó:  0.0 en:  1 8\n",
      "muchas NOT IN text_03\n",
      "se agregó:  0.0 en:  2 8\n",
      "felicidades IN text_01\n",
      "se agregó:  1.0 en:  0 9\n",
      "felicidades NOT IN text_02\n",
      "se agregó:  0.0 en:  1 9\n",
      "felicidades NOT IN text_03\n",
      "se agregó:  0.0 en:  2 9\n",
      "poniendo IN text_01\n",
      "se agregó:  1.0 en:  0 10\n",
      "poniendo NOT IN text_02\n",
      "se agregó:  0.0 en:  1 10\n",
      "poniendo NOT IN text_03\n",
      "se agregó:  0.0 en:  2 10\n",
      "país IN text_01\n",
      "se agregó:  1.0 en:  0 11\n",
      "país NOT IN text_02\n",
      "se agregó:  0.0 en:  1 11\n",
      "país NOT IN text_03\n",
      "se agregó:  0.0 en:  2 11\n",
      "alto IN text_01\n",
      "se agregó:  1.0 en:  0 12\n",
      "alto NOT IN text_02\n",
      "se agregó:  0.0 en:  1 12\n",
      "alto NOT IN text_03\n",
      "se agregó:  0.0 en:  2 12\n",
      "1 NOT IN text_01\n",
      "se agregó:  0.0 en:  0 13\n",
      "1 IN text_02\n",
      "se agregó:  1.0 en:  1 13\n",
      "1 NOT IN text_03\n",
      "se agregó:  0.0 en:  2 13\n",
      "año NOT IN text_01\n",
      "se agregó:  0.0 en:  0 14\n",
      "año IN text_02\n",
      "se agregó:  1.0 en:  1 14\n",
      "año NOT IN text_03\n",
      "se agregó:  0.0 en:  2 14\n",
      "5 NOT IN text_01\n",
      "se agregó:  0.0 en:  0 15\n",
      "5 IN text_02\n",
      "se agregó:  1.0 en:  1 15\n",
      "5 NOT IN text_03\n",
      "se agregó:  0.0 en:  2 15\n",
      "meses NOT IN text_01\n",
      "se agregó:  0.0 en:  0 16\n",
      "meses IN text_02\n",
      "se agregó:  1.0 en:  1 16\n",
      "meses NOT IN text_03\n",
      "se agregó:  0.0 en:  2 16\n",
      "novio NOT IN text_01\n",
      "se agregó:  0.0 en:  0 17\n",
      "novio IN text_02\n",
      "se agregó:  1.0 en:  1 17\n",
      "novio NOT IN text_03\n",
      "se agregó:  0.0 en:  2 17\n",
      "nunca NOT IN text_01\n",
      "se agregó:  0.0 en:  0 18\n",
      "nunca IN text_02\n",
      "se agregó:  1.0 en:  1 18\n",
      "nunca NOT IN text_03\n",
      "se agregó:  0.0 en:  2 18\n",
      "querido NOT IN text_01\n",
      "se agregó:  0.0 en:  0 19\n",
      "querido IN text_02\n",
      "se agregó:  1.0 en:  1 19\n",
      "querido NOT IN text_03\n",
      "se agregó:  0.0 en:  2 19\n",
      "subir NOT IN text_01\n",
      "se agregó:  0.0 en:  0 20\n",
      "subir IN text_02\n",
      "se agregó:  1.0 en:  1 20\n",
      "subir NOT IN text_03\n",
      "se agregó:  0.0 en:  2 20\n",
      "foto NOT IN text_01\n",
      "se agregó:  0.0 en:  0 21\n",
      "foto IN text_02\n",
      "se agregó:  1.0 en:  1 21\n",
      "foto NOT IN text_03\n",
      "se agregó:  0.0 en:  2 21\n",
      "conmigo NOT IN text_01\n",
      "se agregó:  0.0 en:  0 22\n",
      "conmigo IN text_02\n",
      "se agregó:  1.0 en:  1 22\n",
      "conmigo NOT IN text_03\n",
      "se agregó:  0.0 en:  2 22\n",
      "ninguna NOT IN text_01\n",
      "se agregó:  0.0 en:  0 23\n",
      "ninguna IN text_02\n",
      "se agregó:  1.0 en:  1 23\n",
      "ninguna NOT IN text_03\n",
      "se agregó:  0.0 en:  2 23\n",
      "red NOT IN text_01\n",
      "se agregó:  0.0 en:  0 24\n",
      "red IN text_02\n",
      "se agregó:  1.0 en:  1 24\n",
      "red NOT IN text_03\n",
      "se agregó:  0.0 en:  2 24\n",
      "social NOT IN text_01\n",
      "se agregó:  0.0 en:  0 25\n",
      "social IN text_02\n",
      "se agregó:  1.0 en:  1 25\n",
      "social NOT IN text_03\n",
      "se agregó:  0.0 en:  2 25\n",
      "cada NOT IN text_01\n",
      "se agregó:  0.0 en:  0 26\n",
      "cada IN text_02\n",
      "se agregó:  1.0 en:  1 26\n",
      "cada NOT IN text_03\n",
      "se agregó:  0.0 en:  2 26\n",
      "like NOT IN text_01\n",
      "se agregó:  0.0 en:  0 27\n",
      "like IN text_02\n",
      "se agregó:  1.0 en:  1 27\n",
      "like NOT IN text_03\n",
      "se agregó:  0.0 en:  2 27\n",
      "amiga NOT IN text_01\n",
      "se agregó:  0.0 en:  0 28\n",
      "amiga IN text_02\n",
      "se agregó:  1.0 en:  1 28\n",
      "amiga NOT IN text_03\n",
      "se agregó:  0.0 en:  2 28\n",
      "date NOT IN text_01\n",
      "se agregó:  0.0 en:  0 29\n",
      "date IN text_02\n",
      "se agregó:  1.0 en:  1 29\n",
      "date NOT IN text_03\n",
      "se agregó:  0.0 en:  2 29\n",
      "cuenta NOT IN text_01\n",
      "se agregó:  0.0 en:  0 30\n",
      "cuenta IN text_02\n",
      "se agregó:  1.0 en:  1 30\n",
      "cuenta NOT IN text_03\n",
      "se agregó:  0.0 en:  2 30\n",
      "tasa NOT IN text_01\n",
      "se agregó:  0.0 en:  0 31\n",
      "tasa NOT IN text_02\n",
      "se agregó:  0.0 en:  1 31\n",
      "tasa IN text_03\n",
      "se agregó:  1.0 en:  2 31\n",
      "mortalidad NOT IN text_01\n",
      "se agregó:  0.0 en:  0 32\n",
      "mortalidad NOT IN text_02\n",
      "se agregó:  0.0 en:  1 32\n",
      "mortalidad IN text_03\n",
      "se agregó:  1.0 en:  2 32\n",
      "feminicidio NOT IN text_01\n",
      "se agregó:  0.0 en:  0 33\n",
      "feminicidio NOT IN text_02\n",
      "se agregó:  0.0 en:  1 33\n",
      "feminicidio IN text_03\n",
      "se agregó:  1.0 en:  2 33\n",
      "creció NOT IN text_01\n",
      "se agregó:  0.0 en:  0 34\n",
      "creció NOT IN text_02\n",
      "se agregó:  0.0 en:  1 34\n",
      "creció IN text_03\n",
      "se agregó:  1.0 en:  2 34\n",
      "138 NOT IN text_01\n",
      "se agregó:  0.0 en:  0 35\n",
      "138 NOT IN text_02\n",
      "se agregó:  0.0 en:  1 35\n",
      "138 IN text_03\n",
      "se agregó:  1.0 en:  2 35\n",
      "coronavirus NOT IN text_01\n",
      "se agregó:  0.0 en:  0 36\n",
      "coronavirus NOT IN text_02\n",
      "se agregó:  0.0 en:  1 36\n",
      "coronavirus IN text_03\n",
      "se agregó:  1.0 en:  2 36\n",
      "2 NOT IN text_01\n",
      "se agregó:  0.0 en:  0 37\n",
      "2 NOT IN text_02\n",
      "se agregó:  0.0 en:  1 37\n",
      "2 IN text_03\n",
      "se agregó:  1.0 en:  2 37\n",
      "si NOT IN text_01\n",
      "se agregó:  0.0 en:  0 38\n",
      "si NOT IN text_02\n",
      "se agregó:  0.0 en:  1 38\n",
      "si IN text_03\n",
      "se agregó:  1.0 en:  2 38\n",
      "hoy NOT IN text_01\n",
      "se agregó:  0.0 en:  0 39\n",
      "hoy NOT IN text_02\n",
      "se agregó:  0.0 en:  1 39\n",
      "hoy IN text_03\n",
      "se agregó:  1.0 en:  2 39\n",
      "noche NOT IN text_01\n",
      "se agregó:  0.0 en:  0 40\n",
      "noche NOT IN text_02\n",
      "se agregó:  0.0 en:  1 40\n",
      "noche IN text_03\n",
      "se agregó:  1.0 en:  2 40\n",
      "besuquearas NOT IN text_01\n",
      "se agregó:  0.0 en:  0 41\n",
      "besuquearas NOT IN text_02\n",
      "se agregó:  0.0 en:  1 41\n",
      "besuquearas IN text_03\n",
      "se agregó:  1.0 en:  2 41\n",
      "guey NOT IN text_01\n",
      "se agregó:  0.0 en:  0 42\n",
      "guey NOT IN text_02\n",
      "se agregó:  0.0 en:  1 42\n",
      "guey IN text_03\n",
      "se agregó:  1.0 en:  2 42\n",
      "infectado NOT IN text_01\n",
      "se agregó:  0.0 en:  0 43\n",
      "infectado NOT IN text_02\n",
      "se agregó:  0.0 en:  1 43\n",
      "infectado IN text_03\n",
      "se agregó:  1.0 en:  2 43\n",
      "posibilidades NOT IN text_01\n",
      "se agregó:  0.0 en:  0 44\n",
      "posibilidades NOT IN text_02\n",
      "se agregó:  0.0 en:  1 44\n",
      "posibilidades IN text_03\n",
      "se agregó:  1.0 en:  2 44\n",
      "morir NOT IN text_01\n",
      "se agregó:  0.0 en:  0 45\n",
      "morir NOT IN text_02\n",
      "se agregó:  0.0 en:  1 45\n",
      "morir IN text_03\n",
      "se agregó:  1.0 en:  2 45\n",
      "asesinada NOT IN text_01\n",
      "se agregó:  0.0 en:  0 46\n",
      "asesinada NOT IN text_02\n",
      "se agregó:  0.0 en:  1 46\n",
      "asesinada IN text_03\n",
      "se agregó:  1.0 en:  2 46\n",
      "tipo NOT IN text_01\n",
      "se agregó:  0.0 en:  0 47\n",
      "tipo NOT IN text_02\n",
      "se agregó:  0.0 en:  1 47\n",
      "tipo IN text_03\n",
      "se agregó:  1.0 en:  2 47\n",
      "virus NOT IN text_01\n",
      "se agregó:  0.0 en:  0 48\n",
      "virus NOT IN text_02\n",
      "se agregó:  0.0 en:  1 48\n",
      "virus IN text_03\n",
      "se agregó:  1.0 en:  2 48\n",
      "buenos NOT IN text_01\n",
      "se agregó:  0.0 en:  0 49\n",
      "buenos NOT IN text_02\n",
      "se agregó:  0.0 en:  1 49\n",
      "buenos IN text_03\n",
      "se agregó:  1.0 en:  2 49\n",
      "días NOT IN text_01\n",
      "se agregó:  0.0 en:  0 50\n",
      "días NOT IN text_02\n",
      "se agregó:  0.0 en:  1 50\n",
      "días IN text_03\n",
      "se agregó:  1.0 en:  2 50\n",
      "república NOT IN text_01\n",
      "se agregó:  0.0 en:  0 51\n",
      "república NOT IN text_02\n",
      "se agregó:  0.0 en:  1 51\n",
      "república IN text_03\n",
      "se agregó:  1.0 en:  2 51\n",
      "mexicana NOT IN text_01\n",
      "se agregó:  0.0 en:  0 52\n",
      "mexicana NOT IN text_02\n",
      "se agregó:  0.0 en:  1 52\n",
      "mexicana IN text_03\n",
      "se agregó:  1.0 en:  2 52\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "j = 0\n",
    "\n",
    "for word_termns in dicc_termns: #dicc_termns todos los términos\n",
    "#    print()\n",
    "    for word_texts in dicc_texts: #dicc_texts todos los textos\n",
    "#        print(\"EVALUAR:\", word_termns, \"EN: \", word_texts)\n",
    "        if(word_termns in dicc_texts[word_texts]): #si está\n",
    "            print(word_termns, \"IN\", word_texts)\n",
    "            \n",
    "            matrix[j, i] = 1\n",
    "            \n",
    "        elif(word_termns not in dicc_texts[word_texts]): # si no está\n",
    "            print(word_termns, \"NOT IN\", word_texts)\n",
    "            \n",
    "            matrix[j, i] = 0\n",
    "            \n",
    "            \n",
    "        print(\"se agregó: \", matrix[j,i], \"en: \", j, i)\n",
    "            \n",
    "        j = j + 1\n",
    "        \n",
    "    j = 0\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 53)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0.])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin_cos_t01_t02 = dot(matrix[0],matrix[1])/(norm(matrix[0])*norm(matrix[1]))\n",
    "bin_cos_t01_t02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin_cos_t02_t03 = dot(matrix[1],matrix[2])/(norm(matrix[1])*norm(matrix[2]))\n",
    "bin_cos_t02_t03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin_cos_t01_t03 = dot(matrix[0],matrix[2])/(norm(matrix[0])*norm(matrix[2]))\n",
    "bin_cos_t01_t03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
